<!--
SPDX-License-Identifier: MIT AND CC-BY-4.0
Copyright (c) 2025 John William Creighton (s243a)
-->

# Chapter 6: Shell Pipeline Orchestration

## Overview

Individual scripts are useful, but the real power comes from composing them into pipelines. This chapter covers:

- Multi-stage pipeline generation
- Step configuration
- Input/output handling
- Error propagation
- Real-world examples

## The Pipeline Generator

```prolog
generate_pipeline(
    Steps,      % List of step/4 terms
    Options,    % Pipeline options
    Script      % Generated orchestration script
).
```

### Step Format

```prolog
step(Name, Target, File, StepOptions)
```

- `Name` - Identifier for the step
- `Target` - awk, python, bash, go, rust
- `File` - Path to the script/binary
- `StepOptions` - Per-step configuration

## Basic Pipeline

```prolog
?- generate_pipeline(
    [
        step(filter, awk, 'filter.awk', []),
        step(transform, python, 'transform.py', []),
        step(aggregate, awk, 'aggregate.awk', [])
    ],
    [],
    Script
).
```

Generated:

```bash
#!/bin/bash
# Generated by UnifyWeaver shell_glue
set -euo pipefail

awk -f "filter.awk" \
    | python3 "transform.py" \
    | awk -f "aggregate.awk"
```

## Pipeline with Input/Output

```prolog
generate_pipeline(
    [
        step(parse, awk, 'parse.awk', []),
        step(analyze, python, 'analyze.py', [])
    ],
    [input('data.tsv'), output('results.tsv')],
    Script
).
```

Generated:

```bash
#!/bin/bash
set -euo pipefail

cat "data.tsv" \
    | awk -f "parse.awk" \
    | python3 "analyze.py" \
    > "results.tsv"
```

## Step Options

### Format Specification

```prolog
step(transform, python, 'transform.py', [format(json)])
```

When formats differ between steps, converters are inserted:

```prolog
generate_pipeline(
    [
        step(filter, awk, 'filter.awk', [format(tsv)]),
        step(enrich, python, 'enrich.py', [format(json)]),
        step(output, awk, 'output.awk', [format(tsv)])
    ],
    [],
    Script
).
```

Generated (with converters):

```bash
#!/bin/bash
set -euo pipefail

awk -f "filter.awk" \
    | awk -F'\t' 'BEGIN{OFS=""} {printf "{\"f1\":\"%s\",\"f2\":\"%s\"}\n",$1,$2}' \
    | python3 "enrich.py" \
    | python3 -c 'import sys,json; [print("\t".join(str(v) for v in json.loads(l).values())) for l in sys.stdin]' \
    | awk -f "output.awk"
```

### Parallel Execution

For independent branches:

```prolog
generate_pipeline(
    [
        step(split, bash, 'split.sh', [parallel([
            branch(a, [step(proc_a, awk, 'a.awk', [])]),
            branch(b, [step(proc_b, python, 'b.py', [])])
        ])]),
        step(merge, bash, 'merge.sh', [])
    ],
    [],
    Script
).
```

## Error Handling

### Default: Fail Fast

```bash
set -euo pipefail
```

This means:
- `set -e` - Exit on any command failure
- `set -u` - Error on undefined variables
- `set -o pipefail` - Pipeline fails if any stage fails

### Custom Error Handling

```prolog
generate_pipeline(
    Steps,
    [error_handling(log_continue)],
    Script
).
```

Generated:

```bash
#!/bin/bash

# Log errors but continue
exec 2> >(tee -a pipeline_errors.log >&2)

awk -f "filter.awk" 2>&1 \
    | python3 "transform.py" 2>&1 \
    | awk -f "aggregate.awk"

if [ -s pipeline_errors.log ]; then
    echo "Pipeline completed with errors. See pipeline_errors.log"
    exit 1
fi
```

## Environment Variables

```prolog
generate_pipeline(
    Steps,
    [
        env([
            ('INPUT_FORMAT', 'json'),
            ('DEBUG', 'true')
        ])
    ],
    Script
).
```

Generated:

```bash
#!/bin/bash
set -euo pipefail

export INPUT_FORMAT="json"
export DEBUG="true"

awk -f "filter.awk" \
    | python3 "transform.py"
```

## Temporary Files

For pipelines that need intermediate storage:

```prolog
generate_pipeline(
    [
        step(parse, awk, 'parse.awk', [output_temp(parsed)]),
        step(sort, bash, 'sort -k1', [input_temp(parsed), output_temp(sorted)]),
        step(aggregate, awk, 'aggregate.awk', [input_temp(sorted)])
    ],
    [],
    Script
).
```

Generated:

```bash
#!/bin/bash
set -euo pipefail

# Create temp directory
TMPDIR=$(mktemp -d)
trap "rm -rf $TMPDIR" EXIT

# Stage 1: Parse
awk -f "parse.awk" > "$TMPDIR/parsed"

# Stage 2: Sort
sort -k1 < "$TMPDIR/parsed" > "$TMPDIR/sorted"

# Stage 3: Aggregate
awk -f "aggregate.awk" < "$TMPDIR/sorted"
```

## Real-World Example: Log Analysis Pipeline

```prolog
generate_log_analysis_pipeline(Script) :-
    generate_pipeline(
        [
            % Parse Apache logs
            step(parse, awk, 'parse_apache.awk', [
                format(tsv)
            ]),

            % Filter errors (4xx, 5xx)
            step(filter_errors, awk, 'filter_errors.awk', [
                format(tsv)
            ]),

            % Enrich with GeoIP
            step(geoip, python, 'geoip_lookup.py', [
                format(json)  % Need structured data for GeoIP
            ]),

            % Aggregate by country
            step(aggregate, python, 'aggregate_country.py', [
                format(json)
            ]),

            % Format report
            step(report, awk, 'format_report.awk', [
                format(tsv)
            ])
        ],
        [
            input('/var/log/apache2/access.log'),
            output('error_report.tsv'),
            error_handling(log_continue)
        ],
        Script
    ).
```

Generated:

```bash
#!/bin/bash
set -euo pipefail

exec 2> >(tee -a pipeline_errors.log >&2)

cat "/var/log/apache2/access.log" \
    | awk -f "parse_apache.awk" \
    | awk -f "filter_errors.awk" \
    | awk -F'\t' '{printf "{\"ip\":\"%s\",\"timestamp\":\"%s\",\"status\":%s}\n",$1,$2,$3}' \
    | python3 "geoip_lookup.py" \
    | python3 "aggregate_country.py" \
    | python3 -c 'import sys,json; [print("\t".join(str(v) for v in json.loads(l).values())) for l in sys.stdin]' \
    | awk -f "format_report.awk" \
    > "error_report.tsv"
```

## Conditional Steps

```prolog
generate_pipeline(
    [
        step(parse, awk, 'parse.awk', []),
        step(filter, awk, 'filter.awk', [
            condition('[ "$FILTER_ENABLED" = "true" ]')
        ]),
        step(output, awk, 'output.awk', [])
    ],
    [],
    Script
).
```

Generated:

```bash
#!/bin/bash
set -euo pipefail

if [ "$FILTER_ENABLED" = "true" ]; then
    awk -f "parse.awk" \
        | awk -f "filter.awk" \
        | awk -f "output.awk"
else
    awk -f "parse.awk" \
        | awk -f "output.awk"
fi
```

## Debugging Pipelines

### Tap Points

```prolog
generate_pipeline(
    [
        step(parse, awk, 'parse.awk', [tap('parsed.tsv')]),
        step(transform, python, 'transform.py', [tap('transformed.tsv')]),
        step(output, awk, 'output.awk', [])
    ],
    [debug(true)],
    Script
).
```

Generated:

```bash
#!/bin/bash
set -euo pipefail

awk -f "parse.awk" \
    | tee "parsed.tsv" \
    | python3 "transform.py" \
    | tee "transformed.tsv" \
    | awk -f "output.awk"
```

### Progress Reporting

```prolog
generate_pipeline(
    Steps,
    [progress(true)],
    Script
).
```

Generated:

```bash
#!/bin/bash
set -euo pipefail

awk -f "parse.awk" \
    | pv -l -N "parse" \
    | python3 "transform.py" \
    | pv -l -N "transform" \
    | awk -f "output.awk"
```

## Performance Optimization

### Buffer Tuning

```prolog
generate_pipeline(
    [
        step(fast, awk, 'fast.awk', [buffer(block(1048576))]),  % 1MB
        step(slow, python, 'slow.py', [buffer(line)])
    ],
    [],
    Script
).
```

### GNU Parallel Integration

```prolog
generate_pipeline(
    [
        step(process, python, 'process.py', [
            parallel(jobs(8), chunk(1000))
        ])
    ],
    [],
    Script
).
```

Generated:

```bash
#!/bin/bash
set -euo pipefail

cat input.tsv \
    | parallel --pipe -j8 -N1000 python3 process.py
```

## Pipeline Composition

Pipelines can be composed from sub-pipelines:

```prolog
% Define reusable pipeline fragments
parse_fragment([
    step(parse, awk, 'parse.awk', []),
    step(validate, python, 'validate.py', [])
]).

transform_fragment([
    step(enrich, python, 'enrich.py', []),
    step(normalize, awk, 'normalize.awk', [])
]).

% Compose
full_pipeline(Script) :-
    parse_fragment(Parse),
    transform_fragment(Transform),
    append(Parse, Transform, AllSteps),
    generate_pipeline(AllSteps, [], Script).
```

## Chapter Summary

- **Pipelines** chain scripts with automatic connection
- **Format conversion** is inserted when needed
- **Error handling** is configurable (fail-fast or continue)
- **Temporary files** for non-streamable operations
- **Debugging** with taps and progress reporting
- **Optimization** with buffering and parallel

## Next Steps

In Chapter 7, we'll move to .NET integration:
- PowerShell bridge generation
- IronPython hosting
- In-process communication

## Exercises

1. **Basic pipeline**: Generate a pipeline that:
   - Reads CSV
   - Filters rows where column 3 > 100
   - Outputs JSON

2. **Error handling**: Create a pipeline with `log_continue` that processes a malformed log file.

3. **Parallel processing**: Generate a pipeline that processes files in parallel using GNU parallel.

4. **Debug pipeline**: Add tap points to debug a failing transformation.

## Code Examples

See `examples/01-hello-pipeline/` for:
- `pipeline.sh` - Generated orchestration
- `filter.awk` - Filtering step
- `transform.py` - Transformation step
- `aggregate.awk` - Aggregation step

---

## Navigation

**‚Üê** [Previous: Chapter 5: Shell Script Generation](05_shell_glue) | [üìñ Book 7: Cross-Target Glue](./) | [Next: Chapter 7: .NET Bridge Generation ‚Üí](07_dotnet_bridges)
