# Chapter 12b: Polyglot and Distributed Services

Building on the service mesh patterns from Chapter 12a, this chapter covers cross-language service communication (polyglot) and distributed service architectures with sharding and replication.

## Part 1: Polyglot Services (Phase 5)

Polyglot services enable seamless communication between services written in different programming languages. UnifyWeaver generates the necessary client code and serialization logic automatically.

### Target Languages

UnifyWeaver supports service generation for multiple target languages:

| Language    | HTTP Client        | Thread Safety      | Registry Pattern     |
|-------------|--------------------|--------------------|----------------------|
| Python      | `urllib.request`   | `threading.Lock`   | Module-level dict    |
| Go          | `net/http`         | `sync.RWMutex`     | Struct with mutex    |
| Rust        | `reqwest`          | `RwLock`           | `lazy_static!` macro |
| JavaScript  | `fetch`/`axios`    | Single-threaded    | Module export        |
| C#          | `HttpClient`       | `lock`             | Singleton pattern    |

### Defining Service Dependencies

Services can declare dependencies on other services, specifying the target language and transport:

```prolog
% dep(ServiceName, Language, Transport)
service(api_gateway, [
    polyglot(true),
    target_language(python),
    depends_on([
        dep(user_service, go, tcp(localhost, 8001)),
        dep(order_service, rust, tcp(localhost, 8002)),
        dep(notification_service, python, tcp(localhost, 8003))
    ])
], [
    receive(Request),
    state_get(cache, Cache),
    respond(Response)
]).
```

### Polyglot Service Options

| Option | Values | Description |
|--------|--------|-------------|
| `polyglot(Bool)` | `true`, `false` | Enable polyglot support |
| `target_language(Lang)` | `python`, `go`, `rust`, `javascript`, `csharp` | Target compilation language |
| `depends_on(Deps)` | List of `dep/3` | Service dependencies |

### Generated Code Architecture

When compiling a polyglot service, UnifyWeaver generates:

1. **ServiceRegistry**: Tracks available services and their endpoints
2. **ServiceClient**: HTTP client for cross-service communication
3. **Serialization**: JSON marshaling/unmarshaling for request/response
4. **Error handling**: Connection failures, timeouts, retries

### Python Polyglot Implementation

```python
# Generated by UnifyWeaver - Target language: python
import json
import threading
import urllib.request
from urllib.error import URLError
from typing import Dict, Any, Optional

class ServiceRegistry:
    """Registry for discovering and connecting to services."""

    def __init__(self):
        self._services: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.RLock()

    def register(self, name: str, host: str, port: int, language: str) -> None:
        """Register a service endpoint."""
        with self._lock:
            self._services[name] = {
                'host': host,
                'port': port,
                'language': language,
                'url': f"http://{host}:{port}"
            }

    def get(self, name: str) -> Optional[Dict[str, Any]]:
        """Get service information by name."""
        with self._lock:
            return self._services.get(name)

    def list_services(self) -> Dict[str, Dict[str, Any]]:
        """List all registered services."""
        with self._lock:
            return dict(self._services)

# Global registry instance
SERVICE_REGISTRY = ServiceRegistry()

class ServiceClient:
    """HTTP client for cross-service communication."""

    def __init__(self, registry: ServiceRegistry, timeout: float = 30.0):
        self.registry = registry
        self.timeout = timeout

    def call_service(self, service_name: str, method: str,
                     payload: Dict[str, Any]) -> Dict[str, Any]:
        """Call a remote service method."""
        service = self.registry.get(service_name)
        if not service:
            raise ValueError(f"Service not found: {service_name}")

        url = f"{service['url']}/{method}"
        data = json.dumps(payload).encode('utf-8')

        request = urllib.request.Request(
            url,
            data=data,
            headers={'Content-Type': 'application/json'},
            method='POST'
        )

        try:
            with urllib.request.urlopen(request, timeout=self.timeout) as response:
                return json.loads(response.read().decode('utf-8'))
        except URLError as e:
            raise ConnectionError(f"Failed to call {service_name}: {e}")

# Initialize dependencies
def init_dependencies():
    """Register dependent services."""
    SERVICE_REGISTRY.register('user_service', 'localhost', 8001, 'go')
    SERVICE_REGISTRY.register('order_service', 'localhost', 8002, 'rust')
    SERVICE_REGISTRY.register('notification_service', 'localhost', 8003, 'python')

# Service client instance
client = ServiceClient(SERVICE_REGISTRY)
```

### Go Polyglot Implementation

```go
// Generated by UnifyWeaver - Target language: go
package main

import (
    "bytes"
    "encoding/json"
    "fmt"
    "io"
    "net/http"
    "sync"
    "time"
)

// ServiceInfo holds information about a registered service
type ServiceInfo struct {
    Host     string
    Port     int
    Language string
    URL      string
}

// ServiceRegistry manages service discovery
type ServiceRegistry struct {
    services map[string]*ServiceInfo
    mu       sync.RWMutex
}

// NewServiceRegistry creates a new service registry
func NewServiceRegistry() *ServiceRegistry {
    return &ServiceRegistry{
        services: make(map[string]*ServiceInfo),
    }
}

// Register adds a service to the registry
func (r *ServiceRegistry) Register(name, host string, port int, language string) {
    r.mu.Lock()
    defer r.mu.Unlock()

    r.services[name] = &ServiceInfo{
        Host:     host,
        Port:     port,
        Language: language,
        URL:      fmt.Sprintf("http://%s:%d", host, port),
    }
}

// Get retrieves service information by name
func (r *ServiceRegistry) Get(name string) (*ServiceInfo, bool) {
    r.mu.RLock()
    defer r.mu.RUnlock()

    info, ok := r.services[name]
    return info, ok
}

// ListServices returns all registered services
func (r *ServiceRegistry) ListServices() map[string]*ServiceInfo {
    r.mu.RLock()
    defer r.mu.RUnlock()

    result := make(map[string]*ServiceInfo)
    for k, v := range r.services {
        result[k] = v
    }
    return result
}

// ServiceClient handles cross-service communication
type ServiceClient struct {
    registry *ServiceRegistry
    client   *http.Client
}

// NewServiceClient creates a new service client
func NewServiceClient(registry *ServiceRegistry, timeout time.Duration) *ServiceClient {
    return &ServiceClient{
        registry: registry,
        client: &http.Client{
            Timeout: timeout,
        },
    }
}

// CallService invokes a method on a remote service
func (c *ServiceClient) CallService(serviceName, method string,
    payload map[string]interface{}) (map[string]interface{}, error) {

    service, ok := c.registry.Get(serviceName)
    if !ok {
        return nil, fmt.Errorf("service not found: %s", serviceName)
    }

    url := fmt.Sprintf("%s/%s", service.URL, method)

    data, err := json.Marshal(payload)
    if err != nil {
        return nil, fmt.Errorf("failed to marshal payload: %w", err)
    }

    resp, err := c.client.Post(url, "application/json", bytes.NewReader(data))
    if err != nil {
        return nil, fmt.Errorf("failed to call %s: %w", serviceName, err)
    }
    defer resp.Body.Close()

    body, err := io.ReadAll(resp.Body)
    if err != nil {
        return nil, fmt.Errorf("failed to read response: %w", err)
    }

    var result map[string]interface{}
    if err := json.Unmarshal(body, &result); err != nil {
        return nil, fmt.Errorf("failed to unmarshal response: %w", err)
    }

    return result, nil
}

// Global registry and client
var (
    Registry = NewServiceRegistry()
    Client   = NewServiceClient(Registry, 30*time.Second)
)

func init() {
    // Register dependent services
    Registry.Register("user_service", "localhost", 8001, "go")
    Registry.Register("order_service", "localhost", 8002, "rust")
}
```

### Rust Polyglot Implementation

```rust
// Generated by UnifyWeaver - Target language: rust
use lazy_static::lazy_static;
use reqwest::blocking::Client;
use serde::{Deserialize, Serialize};
use serde_json::Value;
use std::collections::HashMap;
use std::sync::RwLock;
use std::time::Duration;

#[derive(Clone, Debug)]
pub struct ServiceInfo {
    pub host: String,
    pub port: u16,
    pub language: String,
    pub url: String,
}

pub struct ServiceRegistry {
    services: RwLock<HashMap<String, ServiceInfo>>,
}

impl ServiceRegistry {
    pub fn new() -> Self {
        ServiceRegistry {
            services: RwLock::new(HashMap::new()),
        }
    }

    pub fn register(&self, name: &str, host: &str, port: u16, language: &str) {
        let mut services = self.services.write().unwrap();
        services.insert(
            name.to_string(),
            ServiceInfo {
                host: host.to_string(),
                port,
                language: language.to_string(),
                url: format!("http://{}:{}", host, port),
            },
        );
    }

    pub fn get(&self, name: &str) -> Option<ServiceInfo> {
        let services = self.services.read().unwrap();
        services.get(name).cloned()
    }

    pub fn list_services(&self) -> HashMap<String, ServiceInfo> {
        let services = self.services.read().unwrap();
        services.clone()
    }
}

lazy_static! {
    pub static ref SERVICE_REGISTRY: ServiceRegistry = {
        let registry = ServiceRegistry::new();
        // Register dependent services
        registry.register("user_service", "localhost", 8001, "python");
        registry.register("order_service", "localhost", 8002, "go");
        registry
    };
}

pub struct ServiceClient {
    client: Client,
}

impl ServiceClient {
    pub fn new(timeout: Duration) -> Self {
        ServiceClient {
            client: Client::builder()
                .timeout(timeout)
                .build()
                .expect("Failed to create HTTP client"),
        }
    }

    pub fn call_service(
        &self,
        service_name: &str,
        method: &str,
        payload: &Value,
    ) -> Result<Value, Box<dyn std::error::Error>> {
        let service = SERVICE_REGISTRY
            .get(service_name)
            .ok_or_else(|| format!("Service not found: {}", service_name))?;

        let url = format!("{}/{}", service.url, method);

        let response = self
            .client
            .post(&url)
            .json(payload)
            .send()?;

        let result: Value = response.json()?;
        Ok(result)
    }
}

lazy_static! {
    pub static ref CLIENT: ServiceClient = ServiceClient::new(Duration::from_secs(30));
}
```

### Cross-Language Communication Example

Consider a microservices architecture with services in different languages:

```prolog
% User service in Go
service(user_service, [
    target_language(go),
    transport(http(8001))
], [
    receive(UserId),
    state_get(users, Users),
    respond(User)
]).

% Order service in Rust
service(order_service, [
    target_language(rust),
    transport(http(8002)),
    depends_on([dep(user_service, go, tcp(localhost, 8001))])
], [
    receive(OrderRequest),
    call_service(user_service, get_user, UserId, UserData),
    respond(Order)
]).

% API Gateway in Python
service(api_gateway, [
    target_language(python),
    transport(http(8000)),
    depends_on([
        dep(user_service, go, tcp(localhost, 8001)),
        dep(order_service, rust, tcp(localhost, 8002))
    ])
], [
    receive(Request),
    route_request(Request, Response),
    respond(Response)
]).
```

---

## Part 2: Distributed Services (Phase 6)

Distributed services extend the architecture with data sharding, replication, and consistency management for horizontal scalability.

### Sharding Strategies

UnifyWeaver supports four sharding strategies:

| Strategy | Description | Best For |
|----------|-------------|----------|
| `hash` | Simple hash-based partitioning | Even distribution |
| `range` | Range-based partitioning | Sequential access patterns |
| `consistent_hash` | Consistent hashing with virtual nodes | Elastic scaling |
| `geographic` | Location-based partitioning | Multi-region deployments |

### Consistency Levels

| Level | Description | Trade-off |
|-------|-------------|-----------|
| `eventual` | Eventually consistent reads | Highest availability |
| `strong` | Linearizable reads | Lowest availability |
| `quorum` | Majority agreement | Balanced |
| `causal` | Causally consistent | Good for user sessions |
| `read_your_writes` | Session-level consistency | User experience |

### Distributed Service Definition

```prolog
service(user_store, [
    distributed(true),
    sharding(consistent_hash),
    replication(3),
    consistency(quorum),
    partition_key(user_id),
    transport(http(8080))
], [
    receive(Request),
    route_to_shard(Request, Shard),
    state_get(data, Data),
    respond(Response)
]).
```

### Distributed Service Options

| Option | Values | Description |
|--------|--------|-------------|
| `distributed(Bool)` | `true`, `false` | Enable distributed mode |
| `sharding(Strategy)` | `hash`, `range`, `consistent_hash`, `geographic` | Data partitioning strategy |
| `replication(N)` | Integer > 0 | Number of replicas |
| `consistency(Level)` | `eventual`, `strong`, `quorum`, `causal`, `read_your_writes` | Consistency guarantee |
| `partition_key(Key)` | Atom | Field used for partitioning |

### Python Distributed Implementation

```python
# Generated by UnifyWeaver - Distributed service
import hashlib
import threading
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from bisect import bisect_left

class ShardingStrategy(Enum):
    HASH = "hash"
    RANGE = "range"
    CONSISTENT_HASH = "consistent_hash"
    GEOGRAPHIC = "geographic"

class ConsistencyLevel(Enum):
    EVENTUAL = "eventual"
    STRONG = "strong"
    QUORUM = "quorum"
    CAUSAL = "causal"
    READ_YOUR_WRITES = "read_your_writes"

@dataclass
class ClusterNode:
    """Represents a node in the cluster."""
    node_id: str
    host: str
    port: int
    is_primary: bool = True
    weight: int = 100

class ConsistentHashRing:
    """Consistent hashing implementation with virtual nodes."""

    def __init__(self, virtual_nodes: int = 150):
        self.virtual_nodes = virtual_nodes
        self.ring: List[Tuple[int, str]] = []
        self.nodes: Dict[str, ClusterNode] = {}
        self._lock = threading.RLock()

    def _hash(self, key: str) -> int:
        """Generate hash for a key."""
        return int(hashlib.md5(key.encode()).hexdigest(), 16)

    def add_node(self, node: ClusterNode) -> None:
        """Add a node to the hash ring."""
        with self._lock:
            self.nodes[node.node_id] = node
            for i in range(self.virtual_nodes):
                virtual_key = f"{node.node_id}:{i}"
                hash_value = self._hash(virtual_key)
                self.ring.append((hash_value, node.node_id))
            self.ring.sort(key=lambda x: x[0])

    def remove_node(self, node_id: str) -> None:
        """Remove a node from the hash ring."""
        with self._lock:
            if node_id in self.nodes:
                del self.nodes[node_id]
                self.ring = [(h, n) for h, n in self.ring if n != node_id]

    def get_node(self, key: str) -> Optional[ClusterNode]:
        """Get the node responsible for a key."""
        with self._lock:
            if not self.ring:
                return None

            hash_value = self._hash(key)
            hashes = [h for h, _ in self.ring]
            idx = bisect_left(hashes, hash_value)

            if idx >= len(self.ring):
                idx = 0

            node_id = self.ring[idx][1]
            return self.nodes.get(node_id)

    def get_replicas(self, key: str, count: int) -> List[ClusterNode]:
        """Get multiple nodes for replication."""
        with self._lock:
            if not self.ring:
                return []

            hash_value = self._hash(key)
            hashes = [h for h, _ in self.ring]
            idx = bisect_left(hashes, hash_value)

            replicas = []
            seen_nodes = set()

            for i in range(len(self.ring)):
                ring_idx = (idx + i) % len(self.ring)
                node_id = self.ring[ring_idx][1]

                if node_id not in seen_nodes:
                    seen_nodes.add(node_id)
                    replicas.append(self.nodes[node_id])

                    if len(replicas) >= count:
                        break

            return replicas

class ReplicationManager:
    """Manages data replication across nodes."""

    def __init__(self, replication_factor: int, consistency: ConsistencyLevel):
        self.replication_factor = replication_factor
        self.consistency = consistency
        self._lock = threading.RLock()

    @property
    def write_quorum(self) -> int:
        """Calculate write quorum based on replication factor."""
        return (self.replication_factor // 2) + 1

    @property
    def read_quorum(self) -> int:
        """Calculate read quorum based on consistency level."""
        if self.consistency == ConsistencyLevel.EVENTUAL:
            return 1
        elif self.consistency == ConsistencyLevel.STRONG:
            return self.replication_factor
        elif self.consistency == ConsistencyLevel.QUORUM:
            return (self.replication_factor // 2) + 1
        else:
            return 1

    def replicate_write(self, nodes: List[ClusterNode],
                        data: Any) -> Tuple[bool, int]:
        """Replicate data to nodes, return success and ack count."""
        successful = 0

        for node in nodes[:self.replication_factor]:
            try:
                # In real implementation, this would send to node
                successful += 1
            except Exception:
                pass

        return successful >= self.write_quorum, successful

    def read_with_consistency(self, nodes: List[ClusterNode],
                              key: str) -> Tuple[bool, Any]:
        """Read with consistency level, return success and data."""
        responses = []

        for node in nodes[:self.read_quorum]:
            try:
                # In real implementation, this would read from node
                responses.append({"value": None, "version": 0})
            except Exception:
                pass

        if len(responses) < self.read_quorum:
            return False, None

        # Return most recent version
        responses.sort(key=lambda x: x.get("version", 0), reverse=True)
        return True, responses[0].get("value")

class ShardRouter:
    """Routes requests to appropriate shards."""

    def __init__(self, strategy: ShardingStrategy, partition_key: str):
        self.strategy = strategy
        self.partition_key = partition_key
        self.hash_ring = ConsistentHashRing()
        self._lock = threading.RLock()

    def add_shard(self, node: ClusterNode) -> None:
        """Add a shard node."""
        self.hash_ring.add_node(node)

    def get_shard(self, request: Dict[str, Any]) -> Optional[ClusterNode]:
        """Get the shard for a request based on partition key."""
        key_value = request.get(self.partition_key)
        if key_value is None:
            return None

        return self.hash_ring.get_node(str(key_value))

    def get_shards_for_replication(self, request: Dict[str, Any],
                                   count: int) -> List[ClusterNode]:
        """Get multiple shards for replication."""
        key_value = request.get(self.partition_key)
        if key_value is None:
            return []

        return self.hash_ring.get_replicas(str(key_value), count)

# Service configuration
REPLICATION_FACTOR = 3
CONSISTENCY = ConsistencyLevel.QUORUM
SHARDING = ShardingStrategy.CONSISTENT_HASH
PARTITION_KEY = "user_id"

# Initialize components
shard_router = ShardRouter(SHARDING, PARTITION_KEY)
replication_manager = ReplicationManager(REPLICATION_FACTOR, CONSISTENCY)
```

### Go Distributed Implementation

```go
// Generated by UnifyWeaver - Distributed service
package main

import (
    "crypto/md5"
    "encoding/binary"
    "fmt"
    "sort"
    "sync"
    "sync/atomic"
)

// ShardingStrategy defines how data is partitioned
type ShardingStrategy int

const (
    ShardHash ShardingStrategy = iota
    ShardRange
    ShardConsistentHash
    ShardGeographic
)

// ConsistencyLevel defines read/write consistency guarantees
type ConsistencyLevel int

const (
    ConsistencyEventual ConsistencyLevel = iota
    ConsistencyStrong
    ConsistencyQuorum
    ConsistencyCausal
    ConsistencyReadYourWrites
)

// ClusterNode represents a node in the distributed cluster
type ClusterNode struct {
    NodeID    string
    Host      string
    Port      int
    IsPrimary bool
    Weight    int
    healthy   atomic.Bool
}

// HashRingEntry represents an entry in the consistent hash ring
type hashRingEntry struct {
    hash   uint64
    nodeID string
}

// ConsistentHashRing implements consistent hashing with virtual nodes
type ConsistentHashRing struct {
    virtualNodes int
    ring         []hashRingEntry
    nodes        map[string]*ClusterNode
    mu           sync.RWMutex
}

// NewConsistentHashRing creates a new hash ring
func NewConsistentHashRing(virtualNodes int) *ConsistentHashRing {
    return &ConsistentHashRing{
        virtualNodes: virtualNodes,
        ring:         make([]hashRingEntry, 0),
        nodes:        make(map[string]*ClusterNode),
    }
}

func (r *ConsistentHashRing) hash(key string) uint64 {
    h := md5.Sum([]byte(key))
    return binary.BigEndian.Uint64(h[:8])
}

// AddNode adds a node to the hash ring
func (r *ConsistentHashRing) AddNode(node *ClusterNode) {
    r.mu.Lock()
    defer r.mu.Unlock()

    r.nodes[node.NodeID] = node

    for i := 0; i < r.virtualNodes; i++ {
        virtualKey := fmt.Sprintf("%s:%d", node.NodeID, i)
        hashValue := r.hash(virtualKey)
        r.ring = append(r.ring, hashRingEntry{hash: hashValue, nodeID: node.NodeID})
    }

    sort.Slice(r.ring, func(i, j int) bool {
        return r.ring[i].hash < r.ring[j].hash
    })
}

// RemoveNode removes a node from the hash ring
func (r *ConsistentHashRing) RemoveNode(nodeID string) {
    r.mu.Lock()
    defer r.mu.Unlock()

    delete(r.nodes, nodeID)

    newRing := make([]hashRingEntry, 0, len(r.ring)-r.virtualNodes)
    for _, entry := range r.ring {
        if entry.nodeID != nodeID {
            newRing = append(newRing, entry)
        }
    }
    r.ring = newRing
}

// GetNode returns the node responsible for a key
func (r *ConsistentHashRing) GetNode(key string) *ClusterNode {
    r.mu.RLock()
    defer r.mu.RUnlock()

    if len(r.ring) == 0 {
        return nil
    }

    hashValue := r.hash(key)

    idx := sort.Search(len(r.ring), func(i int) bool {
        return r.ring[i].hash >= hashValue
    })

    if idx >= len(r.ring) {
        idx = 0
    }

    return r.nodes[r.ring[idx].nodeID]
}

// GetReplicas returns multiple nodes for replication
func (r *ConsistentHashRing) GetReplicas(key string, count int) []*ClusterNode {
    r.mu.RLock()
    defer r.mu.RUnlock()

    if len(r.ring) == 0 {
        return nil
    }

    hashValue := r.hash(key)
    idx := sort.Search(len(r.ring), func(i int) bool {
        return r.ring[i].hash >= hashValue
    })

    replicas := make([]*ClusterNode, 0, count)
    seen := make(map[string]bool)

    for i := 0; i < len(r.ring) && len(replicas) < count; i++ {
        ringIdx := (idx + i) % len(r.ring)
        nodeID := r.ring[ringIdx].nodeID

        if !seen[nodeID] {
            seen[nodeID] = true
            replicas = append(replicas, r.nodes[nodeID])
        }
    }

    return replicas
}

// ReplicationManager handles data replication
type ReplicationManager struct {
    ReplicationFactor int
    Consistency       ConsistencyLevel
    mu                sync.RWMutex
}

// NewReplicationManager creates a new replication manager
func NewReplicationManager(factor int, consistency ConsistencyLevel) *ReplicationManager {
    return &ReplicationManager{
        ReplicationFactor: factor,
        Consistency:       consistency,
    }
}

// WriteQuorum returns the number of acks needed for a write
func (rm *ReplicationManager) WriteQuorum() int {
    return (rm.ReplicationFactor / 2) + 1
}

// ReadQuorum returns the number of reads needed based on consistency
func (rm *ReplicationManager) ReadQuorum() int {
    switch rm.Consistency {
    case ConsistencyEventual:
        return 1
    case ConsistencyStrong:
        return rm.ReplicationFactor
    case ConsistencyQuorum:
        return (rm.ReplicationFactor / 2) + 1
    default:
        return 1
    }
}

// ShardRouter routes requests to appropriate shards
type ShardRouter struct {
    Strategy     ShardingStrategy
    PartitionKey string
    HashRing     *ConsistentHashRing
    mu           sync.RWMutex
}

// NewShardRouter creates a new shard router
func NewShardRouter(strategy ShardingStrategy, partitionKey string) *ShardRouter {
    return &ShardRouter{
        Strategy:     strategy,
        PartitionKey: partitionKey,
        HashRing:     NewConsistentHashRing(150),
    }
}

// GetShard returns the shard for a request
func (sr *ShardRouter) GetShard(request map[string]interface{}) *ClusterNode {
    keyValue, ok := request[sr.PartitionKey]
    if !ok {
        return nil
    }

    return sr.HashRing.GetNode(fmt.Sprintf("%v", keyValue))
}

// GetShardsForReplication returns multiple shards for replication
func (sr *ShardRouter) GetShardsForReplication(request map[string]interface{},
    count int) []*ClusterNode {
    keyValue, ok := request[sr.PartitionKey]
    if !ok {
        return nil
    }

    return sr.HashRing.GetReplicas(fmt.Sprintf("%v", keyValue), count)
}

// Configuration
var (
    ReplicationFactor = 3
    Consistency       = ConsistencyQuorum
    Sharding          = ShardConsistentHash
    PartitionKey      = "user_id"
)
```

### Rust Distributed Implementation

```rust
// Generated by UnifyWeaver - Distributed service
use std::collections::{BTreeMap, HashMap};
use std::sync::{Arc, RwLock, atomic::{AtomicU64, AtomicBool, Ordering}};
use md5;

#[derive(Clone, Copy, Debug, PartialEq)]
pub enum ShardingStrategy {
    Hash,
    Range,
    ConsistentHash,
    Geographic,
}

#[derive(Clone, Copy, Debug, PartialEq)]
pub enum ConsistencyLevel {
    Eventual,
    Strong,
    Quorum,
    Causal,
    ReadYourWrites,
}

#[derive(Clone, Debug)]
pub struct ClusterNode {
    pub node_id: String,
    pub host: String,
    pub port: u16,
    pub is_primary: bool,
    pub weight: u32,
}

pub struct ConsistentHashRing {
    virtual_nodes: usize,
    ring: RwLock<BTreeMap<u64, String>>,
    nodes: RwLock<HashMap<String, ClusterNode>>,
}

impl ConsistentHashRing {
    pub fn new(virtual_nodes: usize) -> Self {
        ConsistentHashRing {
            virtual_nodes,
            ring: RwLock::new(BTreeMap::new()),
            nodes: RwLock::new(HashMap::new()),
        }
    }

    fn hash(&self, key: &str) -> u64 {
        let digest = md5::compute(key.as_bytes());
        u64::from_be_bytes(digest[..8].try_into().unwrap())
    }

    pub fn add_node(&self, node: ClusterNode) {
        let mut ring = self.ring.write().unwrap();
        let mut nodes = self.nodes.write().unwrap();

        for i in 0..self.virtual_nodes {
            let virtual_key = format!("{}:{}", node.node_id, i);
            let hash_value = self.hash(&virtual_key);
            ring.insert(hash_value, node.node_id.clone());
        }

        nodes.insert(node.node_id.clone(), node);
    }

    pub fn remove_node(&self, node_id: &str) {
        let mut ring = self.ring.write().unwrap();
        let mut nodes = self.nodes.write().unwrap();

        ring.retain(|_, v| v != node_id);
        nodes.remove(node_id);
    }

    pub fn get_node(&self, key: &str) -> Option<ClusterNode> {
        let ring = self.ring.read().unwrap();
        let nodes = self.nodes.read().unwrap();

        if ring.is_empty() {
            return None;
        }

        let hash_value = self.hash(key);

        let node_id = ring
            .range(hash_value..)
            .next()
            .or_else(|| ring.iter().next())
            .map(|(_, v)| v.clone())?;

        nodes.get(&node_id).cloned()
    }

    pub fn get_replicas(&self, key: &str, count: usize) -> Vec<ClusterNode> {
        let ring = self.ring.read().unwrap();
        let nodes = self.nodes.read().unwrap();

        if ring.is_empty() {
            return Vec::new();
        }

        let hash_value = self.hash(key);
        let mut replicas = Vec::with_capacity(count);
        let mut seen = std::collections::HashSet::new();

        // Start from hash position and go around the ring
        for (_, node_id) in ring.range(hash_value..).chain(ring.iter()) {
            if !seen.contains(node_id) {
                seen.insert(node_id.clone());
                if let Some(node) = nodes.get(node_id) {
                    replicas.push(node.clone());
                }
                if replicas.len() >= count {
                    break;
                }
            }
        }

        replicas
    }
}

pub struct ReplicationManager {
    pub replication_factor: usize,
    pub consistency: ConsistencyLevel,
}

impl ReplicationManager {
    pub fn new(replication_factor: usize, consistency: ConsistencyLevel) -> Self {
        ReplicationManager {
            replication_factor,
            consistency,
        }
    }

    pub fn write_quorum(&self) -> usize {
        (self.replication_factor / 2) + 1
    }

    pub fn read_quorum(&self) -> usize {
        match self.consistency {
            ConsistencyLevel::Eventual => 1,
            ConsistencyLevel::Strong => self.replication_factor,
            ConsistencyLevel::Quorum => (self.replication_factor / 2) + 1,
            _ => 1,
        }
    }
}

pub struct ShardRouter {
    strategy: ShardingStrategy,
    partition_key: String,
    hash_ring: ConsistentHashRing,
}

impl ShardRouter {
    pub fn new(strategy: ShardingStrategy, partition_key: &str) -> Self {
        ShardRouter {
            strategy,
            partition_key: partition_key.to_string(),
            hash_ring: ConsistentHashRing::new(150),
        }
    }

    pub fn get_partition_key(&self) -> &str {
        &self.partition_key
    }

    pub fn route_request(&self, request: &HashMap<String, String>) -> Option<ClusterNode> {
        let key_value = request.get(&self.partition_key)?;
        self.hash_ring.get_node(key_value)
    }

    pub fn get_shards_for_replication(
        &self,
        request: &HashMap<String, String>,
        count: usize,
    ) -> Vec<ClusterNode> {
        match request.get(&self.partition_key) {
            Some(key_value) => self.hash_ring.get_replicas(key_value, count),
            None => Vec::new(),
        }
    }
}

// Configuration constants
pub const REPLICATION_FACTOR: usize = 3;
pub const CONSISTENCY: ConsistencyLevel = ConsistencyLevel::Quorum;
pub const SHARDING: ShardingStrategy = ShardingStrategy::ConsistentHash;
pub const PARTITION_KEY: &str = "user_id";
```

### Example: Distributed User Store

```prolog
% Complete distributed user store service
service(user_store, [
    distributed(true),
    sharding(consistent_hash),
    replication(3),
    consistency(quorum),
    partition_key(user_id),
    transport(http(8080))
], [
    % Create user
    receive(create_user(UserId, UserData)),
    route_to_shard(UserId, Shard),
    replicate_write(Shard, UserData),
    respond(ok(UserId)),

    % Get user
    receive(get_user(UserId)),
    route_to_shard(UserId, Shard),
    read_with_consistency(Shard, UserData),
    respond(UserData),

    % Update user
    receive(update_user(UserId, Updates)),
    route_to_shard(UserId, Shard),
    state_get(users, Current),
    merge(Current, Updates, New),
    replicate_write(Shard, New),
    respond(ok)
]).

% Compile to all targets
:- compile_service_to_python(user_store, PythonCode),
   compile_service_to_go(user_store, GoCode),
   compile_service_to_rust(user_store, RustCode).
```

---

## Combined Architecture

A complete polyglot distributed system might look like:

```
┌─────────────────────────────────────────────────────────────────┐
│                        API Gateway (Python)                       │
│                   polyglot + load_balancing                       │
└───────────────┬──────────────┬──────────────┬───────────────────┘
                │              │              │
    ┌───────────▼───┐  ┌───────▼───┐  ┌───────▼──────┐
    │ User Service  │  │  Order    │  │ Notification │
    │    (Go)       │  │ Service   │  │   Service    │
    │ distributed   │  │  (Rust)   │  │   (Python)   │
    │ sharded       │  │ distributed│  │   stateless  │
    └───────┬───────┘  └─────┬─────┘  └──────────────┘
            │                │
    ┌───────▼────────────────▼───────┐
    │         Cluster Nodes          │
    │  ┌────┐  ┌────┐  ┌────┐       │
    │  │ N1 │  │ N2 │  │ N3 │       │
    │  └────┘  └────┘  └────┘       │
    │  Consistent Hash Ring          │
    │  Replication Factor: 3         │
    │  Consistency: Quorum           │
    └────────────────────────────────┘
```

## Summary

This chapter covered:

**Polyglot Services (Phase 5)**:
- Cross-language service communication
- ServiceRegistry for service discovery
- ServiceClient with HTTP/JSON marshaling
- Thread-safe implementations for Python, Go, and Rust
- Dependency declaration with `depends_on/1`

**Distributed Services (Phase 6)**:
- Sharding strategies: hash, range, consistent_hash, geographic
- Consistent hash ring with virtual nodes
- Replication management with configurable factors
- Consistency levels: eventual, strong, quorum, causal
- Partition key routing for request distribution

## Next Steps

Chapter 12c covers Service Discovery (Phase 7) and Distributed Tracing (Phase 8), completing the client-server architecture with runtime discovery and observability.
